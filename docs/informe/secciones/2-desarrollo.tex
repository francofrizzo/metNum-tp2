\section{Desarrollo}

    \subsection{Métodos utilizados}

        \subsubsection{\emph{PageRank}: autovectores para elaborar un \emph{ranking} de páginas web}

        Cualquier usuario de Internet que haya realizado alguna búsqueda en Google habrá notado que, en la inmensa mayoría de los casos, la información deseada no se encuentra demasiado lejos de los primeros resultados arrojados por el buscador. ¿Cómo es clasificada la ingente cantidad de datos disponibles en la Web para lograr presentar al usuario aquellos que tienen mayor relevancia? El punto clave reside en aprovechar la información que cada página web brinda acerca de las demás a través de los vínculos o \emph{links} existentes entre ellas. La idea en que se basa el criterio adoptado es simple: cuantas más páginas tengan \emph{links} que apunten hacia otra página dada, más alta será la probabilidad de que esta última contenga información relevante; y mucho más lo será si los enlaces provienen de orígenes que son, a su vez, considerados importantes.

        La World Wide Web, como cualquier otra red de páginas unidas a través de enlaces, puede ser considerada como un grafo dirigido, con las primeras representadas por los nodos y los segundos, por las aristas. Como punto de partida para elaborar el \emph{ranking}, recurriremos a una versión modificada de la matriz de adyacencia de este grafo, donde para cada página el peso de los enlaces salientes será inversamente proporcional a su cantidad. Denotemos $n_{j}$ a la cantidad de \emph{links} salientes que posee la página $j$, y $L_i$ al conjunto de páginas que poseen un link que apunta hacia $i$. Entonces, si llamamos $A$ a esta matriz, tenemos que $a_{ij} = \frac{1}{n_{j}}$ si $j \in L_i$, y $a_{ij} = 0$ en caso contrario.

        Nuestro objetivo es calcular un \emph{ranking} $x = (x_1, \cdots, x_n)$, donde $x_i$ es el puntaje asignado a la página $i$. Una manera de hacerlo siguiendo el criterio antes mencionado es planteando el sistema de ecuaciones
        \[ x_i = \sum_{j \in L_i} \frac{x_j}{n_j} \qquad i = 1, \cdots n \]

        Notemos que este sistema corresponde a la representación matricial $Ax = x$. Es decir, el problema se reduce a encontrar un autovector de $A$ asociado al autovalor $1$. ¿Podemos garantizar la existencia de una solución a este sistema, y en tal caso, su unicidad?

        Analizando la matriz $A$, podemos notar que, por un lado, $0 \leq a_{ij} \leq 1 (\forall\ i, j)$, y por otra parte, si $n_j > 0$, entonces $\sum_{i=1}^{n} a_{ij} = 1$, y si, por el contrario, $n_j = 0$, entonces $\sum_{i=1}^{n} a_{ij} = 0$. Conceptualmente, cuando una página tiene \emph{links} salientes, los pesos de los mismos suman $1$, mientras que las columnas cuyos valores suman $0$ corresponden a las páginas que no tienen enlaces salientes (o \emph{nodos colgantes}). Por lo tanto, si consideramos una red sin nodos colgantes, tenemos que $A$ es una matriz estocástica por columnas; esto garantiza la existencia de un autovector asociado al autovalor $1$ y, nos dice, además, que cualquier otro autovalor $\lambda$ de $A$ deberá cumplir $|\lambda| < 1$. El modelo construido representa un proceso de Márkov; como explican Brin y Page, este puede interpretarse como el itinerario de un \emph{navegante aleatorio}, que posicionado en una página $j$ cualquiera, se dirige a cualquiera de sus links con probabilidad $\frac{1}{n_j}$.

        Para resolver el problema de los nodos colgantes, consideramos que cuando el navegante llega a uno de estos nodos, selecciona equiprobablemente una página cualquiera de la red para continuar su recorrido. Matematicamente esto puede representarse con una nueva matriz $P_1 = A + D$, donde $D = v d^t$, con

        \begin{center}\begin{tabular}{ccc}
            $\displaystyle v_i = \frac{1}{n}$ & \qquad y & \qquad $d_i = \begin{cases} 1 & \text{si }n_i = 0 \\ 0 & \text{si no} \end{cases}$
        \end{tabular}\end{center}

        $P_1$ sí es una matriz estocástica por columnas, lo cual nos asegura que encontraremos solución. Más aún, dado que $1$ es el autovalor principal de $P_1$, sabemos que podremos hacerlo aplicando el método de la potencia. Ahora bien, es altamente deseable que la solución hallada sea única, para que las páginas queden clasificadas de manera única. Con el sistema tal y como está planteado, no es difícil encontrar ejemplos en los que existe más de una solución posible (ver \cite{Bryan2006}). Afortunadamente, esto también se resuelve de manera sencilla mediante el agregado al modelo de un \emph{factor de amortiguación} o \emph{teletransportación}\footnote{El nombre \emph{factor de teletransportación} hace referencia aque, desde la interpretación del navegante aleatorio antes mencionada, la alteración que se efectúa al sistema puede interpretarse como agregarle la posibilidad de que en cada paso, con probabilidad $c$, el navegante se ``teletransporte'' a una página elegida al azar, independientemente de la existencia de un \emph{link} que conecte esta con su posición actual.} $c$, $0 < c < 1$, que permite definir

        \[ P_2 = c P_1 + (1 - c) E \]

        \noindent donde $E \in \mathbb{R}^{n \times n}$ tiene $\frac{1}{n}$ en todas sus posiciones. Notemos que, dado que $P_1$ y $E$ son matrices estocásticas por columnas, $P_2$, que es un promedio ponderado de ambas, también lo es; y que, además, $P_2$ tiene todos sus coeficientes estrictamente positivos. Es decir, $P_2$ representa una cadena de Márkov ergódica, y por lo tanto, el sistema $P_2 x = x$ tiene solución única.

        \subsubsection{\emph{GeM}: adaptando \emph{PageRank} para clasificar equipos en ligas deportivas}


    \subsection{Implementación}

    Ambos métodos fueron implementados en lenguaje C++, reutilizando el código en los casos en que fue posible, pero atendiendo a las diferencias en los contextos de aplicación esperados para cada uno, especialmente a la hora de decidir las estructuras de datos utilizadas.

    En los dos casos, una vez leídos los datos de entrada, se crea la matriz correspondiente al sistema a resolver. A continuación, partiendo de un vector de probabilidad uniforme ($v \in \mathbb{R}^n$, con $v_i = \frac{1}{n}$ para $i = 1, \cdots, n$), se aplican sucesivas iteraciones del método de la potencia. Como criterio de detención, se estima la convergencia del método en cada iteración; para esto, se calcula la diferencia Manhattan entre el vector obtenido en cada repetición y en la inmediatamente anterior, y se la compara con un umbral de tolerancia, proporcionado como parámetro al momento de la ejecución. El algoritmo se detiene cuando la distancia obtenida está por debajo del umbral.

    En el caso del algoritmo \emph{PageRank}, se tuvo especialmente en cuenta el hecho de que cabe esperar que, en una red con una estructura similar a la de la World Wide Web, la cantidad de links presentes en cada página sea muy escasa en comparación con el número total de páginas. Esto permite incrementar enormemente la eficiencia al almacenar la información: en particular, la matriz $A$ correspondiente al sistema, la que contiene la información sobre los \emph{links} de la red, es \emph{esparsa}, es decir, gran parte de sus coeficientes son $0$. Dado que las alteraciones posteriores de esta matriz (las efectuadas a fin de solucionar el problema de los nodos colgantes y el de las posibles múltiples soluciones), a partir de las cuales se obtiene la matriz $P_2$, pueden ser aplicadas \emph{ad-hoc} al momento de operar, se decidió aprovechar la ventaja mencionada y limitarse a almacenar en memoria la matriz $A$.

    La representación elegida fue \acr{CSC} (\emph{Compressed Sparse Column}). Esta representación consiste en tres vectores. El primero de ellos (\texttt{vals}) almacena todos los valores no nulos de la matriz, recorriéndola por columnas; el segundo (\texttt{ind\_filas}), de igual longitud que el primero, contiene el índice de la fila en que se encuentra el elemento respectivo en \texttt{vals}; el tercero (\texttt{ptr\_cols}), tiene una entrada por columna, y señala en qué posición de \texttt{vals} se encuentra el primer valor correspondiente a cada una de ellas. Esta representación fue elegida por sobre \acr{DOK} (\emph{Dictionary of Keys}) por considerarla más eficiente, tanto en memoria como temporalmente, y por sobre \acr{CSR} (\emph{Compressed Sparse Row}) porque permite distinguir eficientemente las columnas que solo contienen ceros (que requieren en este caso un tratamiento especial): en la presente implementación, se las representa colocando el valor $-1$ en la posición correspondiente del vector \texttt{ptr\_cols}.