\section{Desarrollo}

    \subsection{Conceptos teóricos}

        En esta sección, enunciamos a modo introductorio los principales conceptos que funcionan como sustento teórico de los métodos numéricos que emplearemos. Nuestro objetivo no es profundizar en los mismos, dado que ya existen excelentes trabajos realizados que abordan cada uno de ellos más en detalle.

        Consideremos un sistema que puede tomar diferentes estados, dentro de un conjunto finito de ellos. Un \emph{proceso estocástico} es una sucesión de estos estados. Si consideramos un caso particular de proceso estocástico, en que la probabilidad de que el sistema se encuentre en un estado dado en el momento $k+1$ queda completamente determinada por su estado en el momento $k$, tenemos una \emph{cadena de Márkov}.

        Supongamos que el sistema tiene $n$ estados posibles, y denominemos $p_{ij}$ a la probabilidad de que el sistema pase del estado $j$ al estado $i$. Entonces podemos acomodar estos valores en una matriz $\mat{P} \in \mathbb{R}^{n \times n}$, que llamaremos \emph{matriz de transición} de la cadena.

        Notemos que todas las entradas de la matriz, dado que indican valores de una probabilidad, son no negativas, y además, para $j = 1, \cdots, n$, tenemos que $\sum_{i=1}^{n} p_{ij} = 1$. A una matriz que cumpla con estas condiciones la denominaremos \emph{matriz estocástica por columnas}, y, como enuncian Bryan y Leise en \cite{Bryan2006}, cumplen con la importante propiedad de tener a $1$ como autovalor dominante; es decir, $1$ es autovalor y además, para cualquier otro $\lambda \in \mathbb{R}$ autovalor de una de estas matrices, se cumple que $|\lambda| < 1$. Si, además, una matriz estocástica cumple que todas sus entradas son estrictamente positivas, entonces podemos afirmar que el espacio de autovectores asociados al autovalor $1$ tiene dimensión $1$.

        En este trabajo, será de interés para nosotros poder calcular este autovector. Se cuentan con métodos muy diversos para realizarlo, pero muchos de ellos son excesivamente costosos, especialmente cuando las matrices son de dimensiones considerables. Afortunadamente, para matrices con las características que acabamos de mencionar, el autovector principal puede calcularse mediante un método iterativo, muy sencillo de formular, conocido como \emph{método de la potencia}. Este método se basa en el siguiente resultado: si $x$ es el autovector principal de una matriz $\mat{A}$, y $v$ es un vector inicial cualquiera, entonces $\lim_{k\to\infty}\mat{A}^k v = x$. Es decir, partiendo de un vector inicial y multiplicando repetidas veces por la matriz $\mat{A}$, el resultado eventualmente convergerá al autovector buscado. Para una demostración de este hecho, como así también una exposición más detallada del proceso, puede consultarse \cite[Sección 3]{Kamvar2003}. Como podrá verse en la sección siguiente, y comprobarse luego experimentalmente, este algoritmo resulta muy apropiado para resolver los problemas planteados en este trabajo, y es capaz de aprovechar características propias de las matrices con las que trataremos para mejorar su eficiencia.

    \subsection{Métodos utilizados}

        \subsubsection{\emph{PageRank}: autovectores para elaborar un \emph{ranking} de páginas web}

        Cualquier usuario de Internet que haya realizado alguna búsqueda en Google habrá notado que, en la inmensa mayoría de los casos, la información deseada no se encuentra demasiado lejos de los primeros resultados arrojados por el buscador. ¿Cómo es clasificada la ingente cantidad de datos disponibles en la Web para lograr presentar al usuario aquellos que tienen mayor relevancia? El punto clave reside en aprovechar la información que cada página web brinda acerca de las demás a través de los vínculos o \emph{links} existentes entre ellas. La idea en que se basa el criterio adoptado es simple: cuantas más páginas tengan \emph{links} que apunten hacia otra página dada, más alta será la probabilidad de que esta última contenga información relevante; y mucho más lo será si los enlaces provienen de orígenes que son, a su vez, considerados importantes.

        La World Wide Web, como cualquier otra red de páginas unidas a través de enlaces, puede ser considerada como un grafo dirigido, con las primeras representadas por los nodos y los segundos, por las aristas. Como punto de partida para elaborar el \emph{ranking}, recurriremos a una versión modificada de la matriz de adyacencia de este grafo, donde para cada página el peso de los enlaces salientes será inversamente proporcional a su cantidad. Denotemos $n_{j}$ a la cantidad de \emph{links} salientes que posee la página $j$, y $L_i$ al conjunto de páginas que poseen un link que apunta hacia $i$. Entonces, si llamamos $\mat{A}$ a esta matriz, tenemos que $a_{ij} = \frac{1}{n_{j}}$ si $j \in L_i$, y $a_{ij} = 0$ en caso contrario.

        Nuestro objetivo es calcular un \emph{ranking} $x = (x_1, \cdots, x_n)$, donde $x_i$ es el puntaje asignado a la página $i$. Una manera de hacerlo siguiendo el criterio antes mencionado es planteando el sistema de ecuaciones
        \[ x_i = \sum_{j \in L_i} \frac{x_j}{n_j} \qquad i = 1, \cdots n \]

        Notemos que este sistema corresponde a la representación matricial $\mat{A}x = x$. Es decir, el problema se reduce a encontrar un autovector de $\mat{A}$ asociado al autovalor $1$. ¿Podemos garantizar la existencia de una solución a este sistema, y en tal caso, su unicidad?

        Analizando la matriz $\mat{A}$, podemos notar que, por un lado, $0 \leq a_{ij} \leq 1 (\forall\ i, j)$, y por otra parte, si $n_j > 0$, entonces $\sum_{i=1}^{n} a_{ij} = 1$, y si, por el contrario, $n_j = 0$, entonces $\sum_{i=1}^{n} a_{ij} = 0$. Conceptualmente, cuando una página tiene \emph{links} salientes, los pesos de los mismos suman $1$, mientras que las columnas cuyos valores suman $0$ corresponden a las páginas que no tienen enlaces salientes (o \emph{nodos colgantes}). Por lo tanto, si consideramos una red sin nodos colgantes, tenemos que $\mat{A}$ es una matriz estocástica por columnas; esto garantiza la existencia de un autovector asociado al autovalor $1$ y, nos dice, además, que cualquier otro autovalor $\lambda$ de $\mat{A}$ deberá cumplir $|\lambda| < 1$. El modelo construido representa un proceso de Márkov; como explican Brin y Page\cite{Brin1998}, este puede interpretarse como el itinerario de un \emph{navegante aleatorio}, que posicionado en una página $j$ cualquiera, se dirige a cualquiera de sus links con probabilidad $\frac{1}{n_j}$.

        Para resolver el problema de los nodos colgantes, consideramos que cuando el navegante llega a uno de estos nodos, selecciona equiprobablemente una página cualquiera de la red para continuar su recorrido. Matematicamente esto puede representarse con una nueva matriz $\mat{P_1} = \mat{A} + \mat{D}$, donde $\mat{D} = v d^t$, con

        \begin{center}\begin{tabular}{ccc}
            $\displaystyle v_i = \frac{1}{n}$ & \qquad y & \qquad $d_i = \begin{cases} 1 & \text{si }n_i = 0 \\ 0 & \text{si no} \end{cases}$
        \end{tabular}\end{center}

        $\mat{P_1}$ sí es una matriz estocástica por columnas, lo cual nos asegura que encontraremos solución. Más aún, dado que $1$ es el autovalor principal de $\mat{P_1}$, sabemos que podremos hacerlo aplicando el método de la potencia. Ahora bien, es altamente deseable que la solución hallada sea única, para que las páginas queden clasificadas de manera única. Con el sistema tal y como está planteado, no es difícil encontrar ejemplos en los que existe más de una solución posible (ver \cite{Bryan2006}). Afortunadamente, esto también se resuelve de manera sencilla mediante el agregado al modelo de un \emph{factor de amortiguación} o \emph{teletransportación}\footnote{El nombre \emph{factor de teletransportación} hace referencia aque, desde la interpretación del navegante aleatorio antes mencionada, la alteración que se efectúa al sistema puede interpretarse como agregarle la posibilidad de que en cada paso, con probabilidad $c$, el navegante se ``teletransporte'' a una página elegida al azar, independientemente de la existencia de un \emph{link} que conecte esta con su posición actual.} $c$, $0 < c < 1$, que permite definir

        \[ \mat{P_2} = c \mat{P_1} + (1 - c) \mat{E} \]

        \noindent donde $\mat{E} \in \mathbb{R}^{n \times n}$ tiene $\frac{1}{n}$ en todas sus posiciones. Notemos que, dado que $\mat{P_1}$ y $\mat{E}$ son matrices estocásticas por columnas, $\mat{P_2}$, que es un promedio ponderado de ambas, también lo es; y que, además, $\mat{P_2}$ tiene todos sus coeficientes estrictamente positivos. Es decir, $\mat{P_2}$ representa una cadena de Márkov ergódica, y por lo tanto, el sistema $\mat{P_2} x = x$ tiene solución única.

        \subsubsection{\emph{GeM}: adaptando \emph{PageRank} para clasificar equipos en ligas deportivas}

        El algoritmo presentado en la sección anterior no solo ha resultado sumamente exitoso para resolver el problema para el que fue ideado; también existen otros problemas que pueden resolverse con la misma idea básica. Como ya mencionamos anteriormente, uno de ellos es la elaboración de \emph{rankings} de equipos para ligas deportivas a partir de los resultados obtenidos en los partidos.

        En Govan et al.\cite{Govan2008}, los autores mencionan algunos métodos empleados para este fin, y los comparan frente a uno desarrollado por ellos mismos: \emph{GeM}, que está fuertemente basado en el algoritmo de \emph{PageRank}. La idea es la siguiente: una temporada de una liga deportiva también puede entenderse como un grafo dirigido. Los nodos representan a los equipos, y una arista entre los nodos $i$ y $j$ quiere decir que el equipo $i$ perdió al menos una vez contra el equipo $j$. El peso de las aristas es proporcional a la diferencia absoluta en el marcador del partido al que representan (la suma de ellas, si se trata de más de un partido), normalizadas de forma tal que la suma de los pesos de todas las aristas salientes de cada nodo sea igual a $1$.

        A partir del grafo anteriormente mencionado, puede construirse la matriz $\mat{A}$ de adyacencia del grafo; y, de la misma manera que en el caso de la clasificación de páginas en buscadores, pueden derivarse de ella las matrices $\mat{P_1}$ y $\mat{P_2}$ para resolver los problemas de los nodos colgantes (que representan, en este caso, a los equipos invictos) y de la posible existencia de más de una solución. El sistema resultante es completamente análogo al obtenido para calcular \emph{PageRank}, por lo que no detallaremos los pormenores matemáticos que permiten encontrar su solución. Sí deben tenerse en cuenta las inevitables diferencias implementativas que se derivarán de los distintos contextos de uso en los que se desempeñarán ambos algoritmos, que trataremos con más profundidad en la próxima sección.

    \subsection{Implementación}

    Ambos métodos fueron implementados en lenguaje C++, reutilizando el código en los casos en que fue posible, pero atendiendo a las diferencias en los contextos de aplicación esperados para cada uno, especialmente a la hora de decidir las estructuras de datos utilizadas.

    En los dos casos, una vez leídos los datos de entrada, se crea la matriz correspondiente al sistema a resolver. A continuación, partiendo de un vector de probabilidad uniforme ($v \in \mathbb{R}^n$, con $v_i = \frac{1}{n}$ para $i = 1, \cdots, n$), se aplican sucesivas iteraciones del método de la potencia. Como criterio de detención, se estima la convergencia del método en cada iteración; para esto, se calcula la diferencia Manhattan entre el vector obtenido en cada repetición y en la inmediatamente anterior, y se la compara con un umbral de tolerancia, proporcionado como parámetro al momento de la ejecución. El algoritmo se detiene cuando la distancia obtenida está por debajo del umbral.

        \subsubsection{Implementación de \emph{PageRank}}

        En el caso del algoritmo \emph{PageRank}, se tuvieron especialmente en cuenta dos hechos: por un lado, el contexto de uso hace esperar que deban manejarse enormes cantidades de datos, por lo que es importante hacer hincapié en la eficiencia de la implementación; por otra parte, en una red con una estructura similar a la de la World Wide Web, cabe esperar que la cantidad de links presentes en cada página sea muy escasa en comparación con el número total de páginas. Esto permite optimizar la forma en la que se almacena la información: en particular, la matriz $\mat{A}$ correspondiente al sistema, que contiene los datos sobre los \emph{links} de la red, es \emph{esparsa}, es decir, gran parte de sus coeficientes son $0$. Dado que las alteraciones posteriores de esta matriz (las efectuadas a fin de solucionar el problema de los nodos colgantes y el de las posibles múltiples soluciones), a partir de las cuales se obtiene la matriz $\mat{P_2}$, pueden ser aplicadas \emph{ad-hoc} al momento de operar, se decidió aprovechar la ventaja mencionada y limitarse a almacenar en memoria la matriz $\mat{A}$.

        La representación elegida fue \acr{CSC} (\emph{Compressed Sparse Column}). Esta representación consiste en tres vectores. El primero de ellos (\texttt{vals}) almacena todos los valores no nulos de la matriz, recorriéndola por columnas; el segundo (\texttt{ind\_filas}), de igual longitud que el primero, contiene el índice de la fila en que se encuentra el elemento respectivo en \texttt{vals}; el tercero (\texttt{ptr\_cols}), tiene una entrada por columna, y señala en qué posición de \texttt{vals} se encuentra el primer valor correspondiente a cada una de ellas. Esta representación fue elegida por sobre \acr{DOK} (\emph{Dictionary of Keys}) por considerarla más eficiente, tanto en memoria como temporalmente, y por sobre \acr{CSR} (\emph{Compressed Sparse Row}) porque permite distinguir eficientemente las columnas que solo contienen ceros (que requieren en este caso un tratamiento especial): en la presente implementación, se las representa colocando el valor $-1$ en la posición correspondiente del vector \texttt{ptr\_cols}.

        El único inconveniente que se encontró con la representación elegida es que resulta muy impráctica a la hora de cargar los datos. Es por eso que como paso intermedio entre la lectura de datos y la creación de la matriz, se utilizó una estructura temporal que se asemeja más al formato de \acr{DOK}: un vector correspondiente a las columnas, cuyas entradas son a su vez vectores conteniendo los índices de la columna respectiva donde deberá colocarse un valor no nulo.

        La implementación del método de la potencia no presentó grandes complicaciones: consiste en un ciclo que inicia con un vector con todas sus componentes iguales a $\frac{1}{n}$ y lo multiplica, en cada iteración, por la matriz del sistema. Implementar este producto implicó algo más de complejidad, ya que la matriz por la que se debe multiplicar es diferente a la almacenada en memoria. La multiplicación se realiza columna a columna, siguiendo el orden natural de la estructura elegida para la matriz. Se utiliza un vector acumulador para calcular el resultado, que se inicializa con todas sus posiciones en $0$. Luego, para $i = 1, \cdots, n$, se utiliza la matriz $\mat{A}$ para calcular el producto entre la columna $i$ de la matriz $\mat{P_2}$ por el valor $v_i$ del vector de entrada, y el resultado se suma al vector acumulador. Para cada columna pueden presentarse dos casos, que deben tratarse de forma distinta:

        \begin{enumerate}
            \item Si la columna $i$ de $\mat{A}$ solo contiene valores nulos (es decir, \texttt{ptr\_cols[i-1]} = \texttt{-1}), tendremos que, para todo $j = 1, \cdots, n$, ${p_2}_{ij} = c (\frac{1}{n}) + (1-c) (\frac{1}{n}) = \frac{1}{n}$. Luego, el producto de esta columna por $v_i$ puede calcularse directamente, y será un vector con todas sus componentes iguales a $\frac{v_i}{n}$.
            \item Si, por el contrario, en la columna $i$ de $\mat{A}$ hay valores no nulos, para $j = 1, \cdots, n$, tendremos que ${p_2}_{ij} = c (a_{ij}) + (1-c) (\frac{1}{n})$. Es decir, el producto de esta columna por $v_1$ puede calcularse en dos pasos: primero, consideramos un vector con todas sus componentes iguales a $(1-c) (\frac{1}{n})$, y luego, para los índices $j$ tales que $a_{ij} \neq 0$, le sumamos $c (a_{ij})$ a la componente correspondiente de dicho vector.
        \end{enumerate}

        \subsubsection{Implementación de \emph{GeM}}

        La implementación del método \emph{GeM} fue considerablemente más simple, debido a que, si bien ya no es válida la hipótesis de que la matriz obtenida será esparsa, sí puede asumirse que la instancias a tratar serán considerablemente menores. Dado que el algoritmo es básicamente el mismo, gran parte del código de \emph{PageRank} pudo reutilizarse; las principiales modificaciones estuvieron en la forma de representación de la matriz y, por consiguiente, en la implementación del producto de esta con un vector.

        Para el almacenamiento de la matriz, se decidió utilizar un vector de vectores (cada uno de estos representando una fila de la matriz), por considerar que brindaba un equilibrio adecuado entre eficiencia y facilidad de implementación. Dado que ahora se guardan en memoria todos los coeficientes, se decidió almacenar $\mat{P_2}$ en lugar de $\mat{A}$. De esta forma, las operaciones que transforman la segunda en la primera solo se hacen una vez, durante la carga de los datos, y se incrementa la simplicidad y la eficiencia de la función encargada de calcular el producto entre la matriz y el vector. Esta última función opera ahora de la forma tradicional, multiplicando cada fila de $\mat{P_2}$ por el vector proporcionado y almacenando el resultado en la componente correspondiente del resultado.
